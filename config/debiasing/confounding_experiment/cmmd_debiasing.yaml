defaults:
  - _self_
  # - override hydra/launcher: joblib

# used to calculate class weights for balanced cross entropy - not used
csv_path: .../final_data.csv
class_weighted: False

debiasing_method: cmmd
scm_type: confounding_experiment

data:
  datamodule:
    _target_: src.data.ukbdata.UKBDataModule
    data_dir: .../diffusion_data.h5
    split_file: .../split.json
    batch_size: 64
    num_workers: 8
    model_type: predictmeta
    p_flip: 0.3
    p_shift: 0.3
    p_affine: 0.3
    p_contrast: 0.3
    crop_images: False
    has_bin_age: True

experiment_dir: debiasing/${scm_type}/${debiasing_method}/${predictor.pred_head}/${predictor.predictor_type}/cmmd_lambda_${predictor.cmmd_lambda}/inverted_bandwidth_${predictor.inverted_bandwidth}/target_${predictor.target}/class_weighted_${class_weighted}/in_channels_${predictor.resnet_cfg.in_channels}_n_outputs_${predictor.resnet_cfg.n_outputs}_n_blocks_${predictor.resnet_cfg.n_blocks}_n_basefilters_${predictor.resnet_cfg.n_basefilters}_lr_start_${predictor.lr_start}_lr_end_${predictor.lr_end}

hydra:
  # launcher:
  #   n_jobs: 1  # Sequential execution but isolated processes
  #   backend: multiprocessing
  #   prefer: processes
  job:
    chdir: True
  sweep:
    dir: results/${scm_type}/${debiasing_method}/
    subdir: ${experiment_dir}/${hydra.job.id}
  sweeper:
    params:
      +predictor.target: age_bin
      +predictor.lr_start: 0.0001
      +predictor.lr_end: 1e-08
      +predictor.resnet_cfg.n_basefilters: 32
      +predictor.inverted_bandwidth: 0.1
      +predictor.cmmd_lambda: 10.0

seed: 3169

predictor:
  predictor_type: resnet_gn
  pred_head: non-linear
  protected_attributes: [sex, site]
  # cmmd_lambda: 1000.0
  # inverted_bandwidth: 0.001
  resnet_cfg:
    in_channels: 1
    n_outputs: 1
    n_blocks: 4
    no_pooling: False

trainer:
  _target_: pytorch_lightning.Trainer
  accelerator: gpu
  devices: 1
  max_epochs: 100
  enable_progress_bar: True
  detect_anomaly: False
  log_every_n_steps: 1
  enable_checkpointing: true
  limit_train_batches: 1.0
  limit_val_batches: 1.0

callbacks:
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: val/loss
    mode: min
    save_top_k: 1
    dirpath: chkpts
    filename: "epoch-{epoch}-val_loss-{val/loss:.4f}"
    verbose: True
    save_last: True
    auto_insert_metric_name: False
  learning_rate_monitor:
    _target_: pytorch_lightning.callbacks.LearningRateMonitor
    logging_interval: 'epoch'

logger:
  wandb:
    _target_: pytorch_lightning.loggers.WandbLogger
    project: "..."
    name: "${experiment_dir}"
    offline: False
