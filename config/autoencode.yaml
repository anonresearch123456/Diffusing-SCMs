defaults:
  - _self_
  - data: data_gen

# optional, if you want to load a checkpoint
checkpoint_path: null

experiment_dir: autoencode/latent_chan_${autoencoder.model.latent_channels}_b_lr_${autoencoder.base_lr}_d_lr_${autoencoder.disc_lr}_perceptual_${autoencoder.perceptual_weight}_adv_${autoencoder.adv_weight}_kl_${autoencoder.kl_weight}_adv_start_${autoencoder.adv_start}

hydra:
  run:
    dir: results/${experiment_dir}
  job:
    chdir: True
  sweep:
    dir: results
    subdir: ${experiment_dir}

seed: 3169

autoencoder:
  _target_: src.models.lightningmodules.autoencoder.AEModule
  base_lr: 0.00005
  disc_lr: 0.0001
  perceptual_weight: 0.002
  adv_weight: 0.005
  kl_weight: 0.00000001
  adv_start: 5
  model:
    _target_: generative.networks.nets.AutoencoderKL
    spatial_dims: 3
    in_channels: 1
    out_channels: 1
    num_channels: [32, 64, 128, 128]
    latent_channels: 12
    num_res_blocks: 2
    attention_levels: [False, False, False, False]
    with_encoder_nonlocal_attn: False
    with_decoder_nonlocal_attn: False
  discriminator:
    _target_: generative.networks.nets.PatchDiscriminator
    spatial_dims: 3
    num_channels: 96
    num_layers_d: 3
    in_channels: 1
  perceptual_loss:
    _target_: generative.losses.perceptual.PerceptualLoss
    spatial_dims: 3
    network_type: "squeeze"
    is_fake_3d: True
    fake_3d_ratio: 0.25
  adv_loss:
    _target_: generative.losses.adversarial_loss.PatchAdversarialLoss
    criterion: "least_squares"
    no_activation_leastsq: True

trainer:
  _target_: pytorch_lightning.Trainer
  accelerator: gpu
  devices: 1
  max_epochs: 20
  enable_progress_bar: True
  detect_anomaly: False
  log_every_n_steps: 1

callbacks:
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: val/loss
    mode: min
    save_top_k: 1
    dirpath: chkpts
    filename: "epoch-{epoch}-val_loss-{val/loss:.4f}"
    verbose: True
    save_last: True
    auto_insert_metric_name: False
  learning_rate_monitor:
    _target_: pytorch_lightning.callbacks.LearningRateMonitor
    logging_interval: 'epoch'

logger:
  wandb:
    _target_: pytorch_lightning.loggers.WandbLogger
    project: "..."
    name: "${experiment_dir}"
    offline: False
